{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "442bd4fb-2712-48e8-946b-fdf92d33fdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c22de69-4947-4fea-abfc-ebf7d07308f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:241: FutureWarning: mne.io.pick.pick_types is deprecated will be removed in 1.6, use documented public API instead. If no appropriate public API exists, please open an issue on GitHub.\n",
      "<frozen importlib._bootstrap>:241: FutureWarning: mne.io.pick.pick_channels_regexp is deprecated will be removed in 1.6, use documented public API instead. If no appropriate public API exists, please open an issue on GitHub.\n",
      "<frozen importlib._bootstrap>:241: FutureWarning: mne.io.pick.channel_type is deprecated will be removed in 1.6, use documented public API instead. If no appropriate public API exists, please open an issue on GitHub.\n",
      "d:\\Programs\\minconda3\\envs\\serpentine\\Lib\\site-packages\\moabb\\pipelines\\__init__.py:26: ModuleNotFoundError: Tensorflow is not installed. You won't be able to use these MOABB pipelines if you attempt to do so.\n",
      "  warn(\n",
      "d:\\Programs\\minconda3\\envs\\serpentine\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import mne\n",
    "from moabb.datasets import BNCI2014_001\n",
    "import numpy as np\n",
    "from ComfyNet import ComfyNet\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from BCIloader import get_bci_data, filter_bci_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a67553f-aa87-41c8-b000-ab19cb5f2220",
   "metadata": {},
   "outputs": [],
   "source": [
    "mne.set_config(\"MNE_DATA\",\"bciData\")\n",
    "\n",
    "bci_subjects_excluded = []\n",
    "bci_subjects = [i for i in range(1, 10) if i not in bci_subjects_excluded]\n",
    "\n",
    "dataset = BNCI2014_001().get_data(subjects=bci_subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "322c38cd-503d-4b33-911f-d57d909f248b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bci_ch_names = dataset[1]['0train']['1'].ch_names\n",
    "bci_n_channels = len(bci_ch_names)\n",
    "bci_n_samples = dataset[1]['0train']['1'].n_times\n",
    "bci_sfreq = 250\n",
    "\n",
    "n_classes_bci = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48c18166-dbe4-4734-a432-90c44233f832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up band-pass filter from 4 - 40 Hz\n",
      "\n",
      "IIR filter parameters\n",
      "---------------------\n",
      "Chebyshev I bandpass non-linear phase (one-pass forward) causal filter:\n",
      "- Filter order 6 (forward)\n",
      "- Cutoffs at 4.00, 40.00 Hz: -1.00, -1.00 dB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  24 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=2)]: Done 55520 tasks      | elapsed:    4.8s\n",
      "[Parallel(n_jobs=2)]: Done 57024 out of 57024 | elapsed:    4.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up band-pass filter from 4 - 40 Hz\n",
      "\n",
      "IIR filter parameters\n",
      "---------------------\n",
      "Chebyshev I bandpass non-linear phase (one-pass forward) causal filter:\n",
      "- Filter order 6 (forward)\n",
      "- Cutoffs at 4.00, 40.00 Hz: -1.00, -1.00 dB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done 510 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done 57024 out of 57024 | elapsed:    3.9s finished\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = get_bci_data(bci_subjects_excluded=[], test=False)\n",
    "X_train = filter_bci_data(X_train, bci_sfreq=250)\n",
    "X_test, y_test = get_bci_data(bci_subjects_excluded=[], test=True)\n",
    "X_test = filter_bci_data(X_test, bci_sfreq=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b5c0476-5f6d-4549-b842-9e217fb0d656",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetWrapped(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        # Convert to torch tensors\n",
    "        self.X = torch.from_numpy(X).float()\n",
    "        self.Y = torch.from_numpy(Y).long()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.X[idx], self.Y[idx]]\n",
    "\n",
    "train_dataset_bci = DatasetWrapped(X_train, y_train)\n",
    "val_dataset_bci = DatasetWrapped(X_test, y_test)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset_bci,batch_size = 32, shuffle = True)\n",
    "test_dataloader = DataLoader(val_dataset_bci,batch_size = 32, shuffle= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de6548bf-4be3-4a4b-aeda-3074456131e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Documents\\Projects\\Serpentine\\comfynet-bci\\utils\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================================================================================================\n",
      "Layer (type (var_name):depth-idx)                            Input Shape               Output Shape              Param #                   Kernel Shape\n",
      "================================================================================================================================================================\n",
      "ComfyNet (ComfyNet)                                          [1, 22, 640]              [1, 2]                    --                        --\n",
      "├─_PatchEmbedding (patch_embedding): 1-1                     [1, 1, 22, 640]           [1, 19, 40]               --                        --\n",
      "│    └─Sequential (shallownet): 2-1                          [1, 1, 22, 640]           [1, 40, 1, 19]            --                        --\n",
      "│    │    └─Conv2d (0): 3-1                                  [1, 1, 22, 640]           [1, 40, 22, 609]          1,320                     [1, 32]\n",
      "│    │    └─Conv2d (1): 3-2                                  [1, 40, 22, 609]          [1, 40, 1, 609]           35,240                    [22, 1]\n",
      "│    │    └─BatchNorm2d (2): 3-3                             [1, 40, 1, 609]           [1, 40, 1, 609]           80                        --\n",
      "│    │    └─ELU (3): 3-4                                     [1, 40, 1, 609]           [1, 40, 1, 609]           --                        --\n",
      "│    │    └─AvgPool2d (4): 3-5                               [1, 40, 1, 609]           [1, 40, 1, 19]            --                        [1, 60]\n",
      "│    │    └─Dropout (5): 3-6                                 [1, 40, 1, 19]            [1, 40, 1, 19]            --                        --\n",
      "│    └─Sequential (projection): 2-2                          [1, 40, 1, 19]            [1, 19, 40]               --                        --\n",
      "│    │    └─Conv2d (0): 3-7                                  [1, 40, 1, 19]            [1, 40, 1, 19]            1,640                     [1, 1]\n",
      "│    │    └─Rearrange (1): 3-8                               [1, 40, 1, 19]            [1, 19, 40]               --                        --\n",
      "├─PositionalEncoding (positional_encoding): 1-2              [1, 19, 40]               [1, 19, 40]               --                        --\n",
      "├─_TransformerEncoder (transformer): 1-3                     [1, 19, 40]               [1, 19, 40]               --                        --\n",
      "│    └─_TransformerEncoderBlock (0): 2-3                     [1, 19, 40]               [1, 19, 40]               --                        --\n",
      "│    │    └─_ResidualAdd (0): 3-9                            [1, 19, 40]               [1, 19, 40]               6,640                     --\n",
      "│    │    └─_ResidualAdd (1): 3-10                           [1, 19, 40]               [1, 19, 40]               13,080                    --\n",
      "│    └─_TransformerEncoderBlock (1): 2-4                     [1, 19, 40]               [1, 19, 40]               --                        --\n",
      "│    │    └─_ResidualAdd (0): 3-11                           [1, 19, 40]               [1, 19, 40]               6,640                     --\n",
      "│    │    └─_ResidualAdd (1): 3-12                           [1, 19, 40]               [1, 19, 40]               13,080                    --\n",
      "├─_FullyConnected (fc): 1-4                                  [1, 19, 40]               [1, 32]                   --                        --\n",
      "│    └─Sequential (fc): 2-5                                  [1, 760]                  [1, 32]                   --                        --\n",
      "│    │    └─Linear (0): 3-13                                 [1, 760]                  [1, 256]                  194,816                   --\n",
      "│    │    └─ELU (1): 3-14                                    [1, 256]                  [1, 256]                  --                        --\n",
      "│    │    └─Dropout (2): 3-15                                [1, 256]                  [1, 256]                  --                        --\n",
      "│    │    └─Linear (3): 3-16                                 [1, 256]                  [1, 32]                   8,224                     --\n",
      "│    │    └─ELU (4): 3-17                                    [1, 32]                   [1, 32]                   --                        --\n",
      "│    │    └─Dropout (5): 3-18                                [1, 32]                   [1, 32]                   --                        --\n",
      "├─_FinalLayer (final_layer): 1-5                             [1, 32]                   [1, 2]                    --                        --\n",
      "│    └─Sequential (final_layer): 2-6                         [1, 32]                   [1, 2]                    --                        --\n",
      "│    │    └─Linear (0): 3-19                                 [1, 32]                   [1, 2]                    66                        --\n",
      "================================================================================================================================================================\n",
      "Total params: 280,826\n",
      "Trainable params: 280,826\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 39.42\n",
      "================================================================================================================================================================\n",
      "Input size (MB): 0.06\n",
      "Forward/backward pass size (MB): 4.82\n",
      "Params size (MB): 1.12\n",
      "Estimated Total Size (MB): 6.00\n",
      "================================================================================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\minconda3\\envs\\serpentine\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ..\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_channels = X_train.shape[1]\n",
    "input_window_samples = X_train.shape[2]\n",
    "\n",
    "model = ComfyNet(\n",
    "    n_outputs=n_classes_bci,\n",
    "    n_chans=len(bci_ch_names[:22]),\n",
    "    n_filters_time=40,\n",
    "    filter_time_length=32,\n",
    "    pool_time_length=60,\n",
    "    pool_time_stride=30,\n",
    "    drop_prob=0.5,\n",
    "    att_depth=2,\n",
    "    att_heads=4,\n",
    "    att_drop_prob=0.5,\n",
    "    return_features=False,\n",
    "    n_times = input_window_samples,\n",
    "    final_fc_length = 760\n",
    ")\n",
    "\n",
    "# Display torchinfo table describing the model\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04a24e1b-532c-458c-a019-72c6b9e1b7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train (model, optimizer, loss, train_dataloader, test_dataloder, epochs = 2, device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
    "\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "            \n",
    "        total_train_loss = 0\n",
    "        train_acc = 0\n",
    "        total_val_loss = 0\n",
    "        val_acc = 0\n",
    "        avg_train_loss = 0\n",
    "        avg_val_loss = 0\n",
    "        avg_train_acc=0\n",
    "        avg_val_acc = 0\n",
    "\n",
    "        trained_samples = 0\n",
    "\n",
    "        pbar = tqdm(train_dataloader)\n",
    "\n",
    "        model.train()\n",
    "        for i, (X, y) in enumerate(pbar):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X)\n",
    "            l = loss(y_pred, y)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += l.item()\n",
    "            train_acc += (y_pred.argmax(1) == y).sum().item()\n",
    "            trained_samples+= X.shape[0]\n",
    "\n",
    "            avg_train_loss = total_train_loss/trained_samples\n",
    "            avg_train_acc = train_acc/trained_samples\n",
    "            pbar.set_description(f\"Epoch {epoch+1}, Loss: {avg_train_loss :.3f}, Accuracy: {avg_train_acc :.3f}\")\n",
    "        \n",
    "        validated_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for X, y in tqdm(test_dataloader):\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                y_pred = model(X)\n",
    "                l = loss(y_pred, y)\n",
    "                total_val_loss += l.item()\n",
    "                validated_samples += X.shape[0]\n",
    "                val_acc += (y_pred.argmax(1) == y).sum().item()\n",
    "                \n",
    "                avg_val_loss = total_val_loss/validated_samples\n",
    "                avg_val_acc = val_acc/validated_samples\n",
    "            print(f\"Validation Loss: {avg_val_loss :.3f}, Validation Accuracy: {avg_val_acc :.3f}\")\n",
    "        \n",
    "        # wandb.log({'train_acc':avg_train_acc,\"train_loss\":avg_train_loss, \"val_loss\" : avg_val_loss, \"val_acc\": avg_val_acc})\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7604edb-5cf8-4f83-8b5b-32bc8259891a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/81 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m)\n\u001b[0;32m      2\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 27\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, loss, train_dataloader, test_dataloder, epochs, device)\u001b[0m\n\u001b[0;32m     25\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model(X)\n\u001b[0;32m     26\u001b[0m l \u001b[38;5;241m=\u001b[39m loss(y_pred, y)\n\u001b[1;32m---> 27\u001b[0m \u001b[43ml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     29\u001b[0m total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m l\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32md:\\Programs\\minconda3\\envs\\serpentine\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\minconda3\\envs\\serpentine\\Lib\\site-packages\\torch\\autograd\\__init__.py:260\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    251\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    252\u001b[0m     (inputs,)\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, (torch\u001b[38;5;241m.\u001b[39mTensor, graph\u001b[38;5;241m.\u001b[39mGradientEdge))\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m()\n\u001b[0;32m    257\u001b[0m )\n\u001b[0;32m    259\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[1;32m--> 260\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[1;32md:\\Programs\\minconda3\\envs\\serpentine\\Lib\\site-packages\\torch\\autograd\\__init__.py:143\u001b[0m, in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[0;32m    137\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    138\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for real scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    139\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    140\u001b[0m         )\n\u001b[0;32m    141\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg)\n\u001b[0;32m    142\u001b[0m     new_grads\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 143\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreserve_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m     )\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m     new_grads\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "train(model, optimizer, loss, train_dataloader, test_dataloader, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d8d1aed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8354978354978355"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('models/model_7.pt')\n",
    "checkpoint['val_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6fd9f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
